{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4914645,"sourceType":"datasetVersion","datasetId":2850202}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# âœ… Correct dataset directory and filenames\nimport os\nimport pandas as pd\nimport numpy as np\n\nDATA_PATH = \"/kaggle/input/fifa-23-complete-player-dataset\"\n\ndef load_csv(filename, large=False, chunksize=250000):\n    path = os.path.join(DATA_PATH, filename)\n    print(f\"\\nðŸ“¥ Loading {filename} ...\")\n    if large:\n        chunks = []\n        for i, chunk in enumerate(pd.read_csv(path, chunksize=chunksize, low_memory=False)):\n            chunks.append(chunk)\n            print(f\"  â†’ Chunk {i+1} loaded ({len(chunk)} rows)\")\n        df = pd.concat(chunks, ignore_index=True)\n    else:\n        df = pd.read_csv(path, low_memory=False)\n    print(f\"âœ… Loaded: {filename} | Shape: {df.shape}\")\n    return df","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-12T14:55:57.474219Z","iopub.execute_input":"2025-11-12T14:55:57.474510Z","iopub.status.idle":"2025-11-12T14:55:57.784816Z","shell.execute_reply.started":"2025-11-12T14:55:57.474487Z","shell.execute_reply":"2025-11-12T14:55:57.784002Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"female_coaches        = load_csv(\"female_coaches.csv\")\nfemale_players_legacy = load_csv(\"female_players (legacy).csv\")\nfemale_players        = load_csv(\"female_players.csv\", large=True)\nfemale_teams          = load_csv(\"female_teams.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T14:55:58.831458Z","iopub.execute_input":"2025-11-12T14:55:58.832315Z","iopub.status.idle":"2025-11-12T14:56:03.998675Z","shell.execute_reply.started":"2025-11-12T14:55:58.832292Z","shell.execute_reply":"2025-11-12T14:56:03.997721Z"}},"outputs":[{"name":"stdout","text":"\nðŸ“¥ Loading female_coaches.csv ...\nâœ… Loaded: female_coaches.csv | Shape: (52, 8)\n\nðŸ“¥ Loading female_players (legacy).csv ...\nâœ… Loaded: female_players (legacy).csv | Shape: (3196, 110)\n\nðŸ“¥ Loading female_players.csv ...\n  â†’ Chunk 1 loaded (181361 rows)\nâœ… Loaded: female_players.csv | Shape: (181361, 110)\n\nðŸ“¥ Loading female_teams.csv ...\nâœ… Loaded: female_teams.csv | Shape: (7941, 54)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"male_coaches          = load_csv(\"male_coaches.csv\")\nmale_players_legacy   = load_csv(\"male_players (legacy).csv\")\nmale_players          = load_csv(\"male_players.csv\", large=True)\nmale_teams            = load_csv(\"male_teams.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T14:56:05.709696Z","iopub.execute_input":"2025-11-12T14:56:05.710296Z","iopub.status.idle":"2025-11-12T15:01:15.658029Z","shell.execute_reply.started":"2025-11-12T14:56:05.710271Z","shell.execute_reply":"2025-11-12T15:01:15.657126Z"}},"outputs":[{"name":"stdout","text":"\nðŸ“¥ Loading male_coaches.csv ...\nâœ… Loaded: male_coaches.csv | Shape: (1155, 8)\n\nðŸ“¥ Loading male_players (legacy).csv ...\nâœ… Loaded: male_players (legacy).csv | Shape: (161583, 110)\n\nðŸ“¥ Loading male_players.csv ...\n  â†’ Chunk 1 loaded (250000 rows)\n  â†’ Chunk 2 loaded (250000 rows)\n  â†’ Chunk 3 loaded (250000 rows)\n  â†’ Chunk 4 loaded (250000 rows)\n  â†’ Chunk 5 loaded (250000 rows)\n  â†’ Chunk 6 loaded (250000 rows)\n  â†’ Chunk 7 loaded (250000 rows)\n  â†’ Chunk 8 loaded (250000 rows)\n  â†’ Chunk 9 loaded (250000 rows)\n  â†’ Chunk 10 loaded (250000 rows)\n  â†’ Chunk 11 loaded (250000 rows)\n  â†’ Chunk 12 loaded (250000 rows)\n  â†’ Chunk 13 loaded (250000 rows)\n  â†’ Chunk 14 loaded (250000 rows)\n  â†’ Chunk 15 loaded (250000 rows)\n  â†’ Chunk 16 loaded (250000 rows)\n  â†’ Chunk 17 loaded (250000 rows)\n  â†’ Chunk 18 loaded (250000 rows)\n  â†’ Chunk 19 loaded (250000 rows)\n  â†’ Chunk 20 loaded (250000 rows)\n  â†’ Chunk 21 loaded (250000 rows)\n  â†’ Chunk 22 loaded (250000 rows)\n  â†’ Chunk 23 loaded (250000 rows)\n  â†’ Chunk 24 loaded (250000 rows)\n  â†’ Chunk 25 loaded (250000 rows)\n  â†’ Chunk 26 loaded (250000 rows)\n  â†’ Chunk 27 loaded (250000 rows)\n  â†’ Chunk 28 loaded (250000 rows)\n  â†’ Chunk 29 loaded (250000 rows)\n  â†’ Chunk 30 loaded (250000 rows)\n  â†’ Chunk 31 loaded (250000 rows)\n  â†’ Chunk 32 loaded (250000 rows)\n  â†’ Chunk 33 loaded (250000 rows)\n  â†’ Chunk 34 loaded (250000 rows)\n  â†’ Chunk 35 loaded (250000 rows)\n  â†’ Chunk 36 loaded (250000 rows)\n  â†’ Chunk 37 loaded (250000 rows)\n  â†’ Chunk 38 loaded (250000 rows)\n  â†’ Chunk 39 loaded (250000 rows)\n  â†’ Chunk 40 loaded (250000 rows)\n  â†’ Chunk 41 loaded (3590 rows)\nâœ… Loaded: male_players.csv | Shape: (10003590, 110)\n\nðŸ“¥ Loading male_teams.csv ...\nâœ… Loaded: male_teams.csv | Shape: (385055, 54)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def preprocess_df(df, drop_cols=None, id_col=None):\n    df=df.copy()\n\n    # basic cleaning\n    df.columns = df.columns.str.strip()\n    df.replace([\"-\", \"NA\", \"N/A\", \"\"], np.nan, inplace=True)\n\n    # drop unwanted columns (URLs, images, etc.)\n    if drop_cols:\n        df.drop(columns=[c for c in drop_cols if c in df.columns], inplace=True)\n\n    # remove duplicates\n    if id_col and id_col in df.columns:\n        before = len(df)\n        df.drop_duplicates(subset=[id_col], keep=\"last\", inplace=True)\n        after = len(df)\n        print(f\" Removed {before - after} duplicates based on '{id_col}'\")\n    else:\n        df.drop_duplicates(inplace=True)\n\n    # downcast numeric types\n    for col in df.select_dtypes(include=[\"int64\", \"float64\"]).columns:\n        df[col] = pd.to_numeric(df[col], downcast=\"integer\", errors=\"ignore\")\n\n    # summary\n    print(f\" cleaned dataframe | shape: {df.shape} | Missing: {df.isna().sum().sum()}\")\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T15:01:24.285258Z","iopub.execute_input":"2025-11-12T15:01:24.285573Z","iopub.status.idle":"2025-11-12T15:01:24.292019Z","shell.execute_reply.started":"2025-11-12T15:01:24.285550Z","shell.execute_reply":"2025-11-12T15:01:24.291264Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"drop_url_cols = [\"player_url\", \"player_face_url\", \"coach_url\", \"face_url\", \"team_url\"]\n\nfemale_coaches_clean = preprocess_df(female_coaches, drop_cols=drop_url_cols, id_col=\"coach_id\")\nfemale_players_clean = preprocess_df(female_players, drop_cols=drop_url_cols, id_col=\"player_id\")\nfemale_players_legacy_clean = preprocess_df(female_players_legacy, drop_cols=drop_url_cols, id_col=\"player_id\")\nfemale_teams_clean = preprocess_df(female_teams, drop_cols=drop_url_cols, id_col=\"team_id\")\n\nmale_coaches_clean = preprocess_df(male_coaches, drop_cols=drop_url_cols, id_col=\"coach_id\")\n# male_players_clean = preprocess_df(male_players, drop_cols=drop_url_cols, id_col=\"player_id\")\nmale_players_legacy_clean = preprocess_df(male_players_legacy, drop_cols=drop_url_cols, id_col=\"player_id\")\nmale_teams_clean = preprocess_df(male_teams, drop_cols=drop_url_cols, id_col=\"team_id\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T15:01:27.056376Z","iopub.execute_input":"2025-11-12T15:01:27.056936Z","iopub.status.idle":"2025-11-12T15:01:31.868780Z","shell.execute_reply.started":"2025-11-12T15:01:27.056908Z","shell.execute_reply":"2025-11-12T15:01:31.867683Z"}},"outputs":[{"name":"stdout","text":" Removed 0 duplicates based on 'coach_id'\n cleaned dataframe | shape: (52, 6) | Missing: 11\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_94/4110948774.py:23: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n  df[col] = pd.to_numeric(df[col], downcast=\"integer\", errors=\"ignore\")\n","output_type":"stream"},{"name":"stdout","text":" Removed 179726 duplicates based on 'player_id'\n cleaned dataframe | shape: (1635, 108) | Missing: 23488\n Removed 1930 duplicates based on 'player_id'\n cleaned dataframe | shape: (1266, 108) | Missing: 17163\n Removed 7889 duplicates based on 'team_id'\n cleaned dataframe | shape: (52, 53) | Missing: 705\n Removed 0 duplicates based on 'coach_id'\n cleaned dataframe | shape: (1155, 6) | Missing: 83\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_94/4110948774.py:23: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n  df[col] = pd.to_numeric(df[col], downcast=\"integer\", errors=\"ignore\")\n/tmp/ipykernel_94/4110948774.py:23: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n  df[col] = pd.to_numeric(df[col], downcast=\"integer\", errors=\"ignore\")\n/tmp/ipykernel_94/4110948774.py:23: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n  df[col] = pd.to_numeric(df[col], downcast=\"integer\", errors=\"ignore\")\n/tmp/ipykernel_94/4110948774.py:23: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n  df[col] = pd.to_numeric(df[col], downcast=\"integer\", errors=\"ignore\")\n","output_type":"stream"},{"name":"stdout","text":" Removed 111884 duplicates based on 'player_id'\n cleaned dataframe | shape: (49699, 108) | Missing: 373009\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_94/4110948774.py:23: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n  df[col] = pd.to_numeric(df[col], downcast=\"integer\", errors=\"ignore\")\n","output_type":"stream"},{"name":"stdout","text":" Removed 383943 duplicates based on 'team_id'\n cleaned dataframe | shape: (1112, 53) | Missing: 12434\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_94/4110948774.py:23: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n  df[col] = pd.to_numeric(df[col], downcast=\"integer\", errors=\"ignore\")\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def clean_large_csv(in_file, out_file, id_col, drop_cols=None, chunksize=500_000):\n    \"\"\"\n    Read and clean a very large CSV in chunks using pandas' C engine.\n    Keeps memory stable and finishes within Kaggle limits.\n    \"\"\"\n    import pandas as pd, numpy as np, os, gc, time\n\n    in_path = os.path.join(DATA_PATH, in_file)\n    out_path = os.path.join(\"/kaggle/working\", out_file)\n\n    # Start fresh\n    if os.path.exists(out_path):\n        os.remove(out_path)\n\n    print(f\"ðŸš€ Cleaning large file in chunks â†’ {in_file}\")\n    start = time.time()\n    total_rows = 0\n\n    reader = pd.read_csv(\n        in_path,\n        chunksize=chunksize,\n        engine=\"c\",             # âœ… supports chunking\n        low_memory=True,\n        na_values=[\"-\", \"NA\", \"N/A\", \"na\", \"\"],\n    )\n\n    for i, chunk in enumerate(reader):\n        # Drop unwanted columns\n        if drop_cols:\n            chunk = chunk.drop(columns=[c for c in drop_cols if c in chunk.columns], errors=\"ignore\")\n\n        # Drop duplicates per chunk\n        if id_col in chunk.columns:\n            chunk = chunk.drop_duplicates(subset=[id_col])\n        else:\n            chunk = chunk.drop_duplicates()\n\n        # Optimize numerics\n        for col in chunk.select_dtypes(include=[\"int64\", \"float64\"]).columns:\n            try:\n                chunk[col] = pd.to_numeric(chunk[col], downcast=\"integer\")\n            except Exception:\n                pass\n\n        # Append cleaned chunk to file\n        chunk.to_csv(out_path, index=False, mode=\"a\", header=not os.path.exists(out_path))\n        total_rows += len(chunk)\n        print(f\"  â†’ processed chunk {i+1}, total {total_rows:,} rows\")\n        del chunk\n        gc.collect()\n\n    print(f\"âœ… Finished {in_file} â†’ {out_file}\")\n    print(f\"ðŸ’¾ Saved to /kaggle/working | Total rows: {total_rows:,} | Time: {round(time.time()-start, 2)}s\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T15:08:15.948808Z","iopub.execute_input":"2025-11-12T15:08:15.949087Z","iopub.status.idle":"2025-11-12T15:08:15.957071Z","shell.execute_reply.started":"2025-11-12T15:08:15.949065Z","shell.execute_reply":"2025-11-12T15:08:15.956350Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"drop_url_cols = [\"player_url\", \"player_face_url\", \"coach_url\", \"face_url\", \"team_url\"]\n\nclean_large_csv(\n    \"male_players.csv\",\n    \"male_players_clean.csv\",\n    id_col=\"player_id\",\n    drop_cols=drop_url_cols,\n    chunksize=750_000  # safe and much faster\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T15:08:25.606089Z","iopub.execute_input":"2025-11-12T15:08:25.606906Z","iopub.status.idle":"2025-11-12T15:11:14.029662Z","shell.execute_reply.started":"2025-11-12T15:08:25.606879Z","shell.execute_reply":"2025-11-12T15:11:14.028851Z"}},"outputs":[{"name":"stdout","text":"ðŸš€ Cleaning large file in chunks â†’ male_players.csv\n  â†’ processed chunk 1, total 26,164 rows\n  â†’ processed chunk 2, total 51,067 rows\n  â†’ processed chunk 3, total 73,139 rows\n  â†’ processed chunk 4, total 98,181 rows\n  â†’ processed chunk 5, total 122,753 rows\n  â†’ processed chunk 6, total 143,375 rows\n  â†’ processed chunk 7, total 166,949 rows\n  â†’ processed chunk 8, total 187,670 rows\n  â†’ processed chunk 9, total 210,604 rows\n  â†’ processed chunk 10, total 229,625 rows\n  â†’ processed chunk 11, total 252,655 rows\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_94/2093646256.py:27: DtypeWarning: Columns (108) have mixed types. Specify dtype option on import or set low_memory=False.\n  for i, chunk in enumerate(reader):\n","output_type":"stream"},{"name":"stdout","text":"  â†’ processed chunk 12, total 273,140 rows\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_94/2093646256.py:27: DtypeWarning: Columns (108) have mixed types. Specify dtype option on import or set low_memory=False.\n  for i, chunk in enumerate(reader):\n","output_type":"stream"},{"name":"stdout","text":"  â†’ processed chunk 13, total 296,621 rows\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_94/2093646256.py:27: DtypeWarning: Columns (108) have mixed types. Specify dtype option on import or set low_memory=False.\n  for i, chunk in enumerate(reader):\n","output_type":"stream"},{"name":"stdout","text":"  â†’ processed chunk 14, total 314,486 rows\nâœ… Finished male_players.csv â†’ male_players_clean.csv\nðŸ’¾ Saved to /kaggle/working | Total rows: 314,486 | Time: 168.42s\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import os\nos.listdir(\"/kaggle/working\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T15:14:40.575085Z","iopub.execute_input":"2025-11-12T15:14:40.575663Z","iopub.status.idle":"2025-11-12T15:14:40.584793Z","shell.execute_reply.started":"2025-11-12T15:14:40.575637Z","shell.execute_reply":"2025-11-12T15:14:40.584081Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"['.virtual_documents', 'male_players_clean.csv']"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# Save all cleaned smaller DataFrames to working directory\ndatasets = {\n    \"female_coaches_clean\": female_coaches,\n    \"female_players_clean\": female_players,\n    \"female_players_legacy_clean\": female_players_legacy,\n    \"female_teams_clean\": female_teams,\n    \"male_coaches_clean\": male_coaches,\n    \"male_players_legacy_clean\": male_players_legacy,\n    \"male_teams_clean\": male_teams\n}\n\nfor name, df in datasets.items():\n    path = f\"/kaggle/working/{name}.csv\"\n    df.to_csv(path, index=False)\n    print(f\"âœ… Saved {name}.csv â†’ /kaggle/working/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T15:15:37.424980Z","iopub.execute_input":"2025-11-12T15:15:37.425798Z","iopub.status.idle":"2025-11-12T15:15:58.568923Z","shell.execute_reply.started":"2025-11-12T15:15:37.425773Z","shell.execute_reply":"2025-11-12T15:15:58.568049Z"}},"outputs":[{"name":"stdout","text":"âœ… Saved female_coaches_clean.csv â†’ /kaggle/working/\nâœ… Saved female_players_clean.csv â†’ /kaggle/working/\nâœ… Saved female_players_legacy_clean.csv â†’ /kaggle/working/\nâœ… Saved female_teams_clean.csv â†’ /kaggle/working/\nâœ… Saved male_coaches_clean.csv â†’ /kaggle/working/\nâœ… Saved male_players_legacy_clean.csv â†’ /kaggle/working/\nâœ… Saved male_teams_clean.csv â†’ /kaggle/working/\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import os\nos.listdir(\"/kaggle/working\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T15:16:07.448232Z","iopub.execute_input":"2025-11-12T15:16:07.448893Z","iopub.status.idle":"2025-11-12T15:16:07.454481Z","shell.execute_reply.started":"2025-11-12T15:16:07.448866Z","shell.execute_reply":"2025-11-12T15:16:07.453773Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"['female_coaches_clean.csv',\n 'female_players_clean.csv',\n 'male_coaches_clean.csv',\n 'female_teams_clean.csv',\n '.virtual_documents',\n 'male_players_legacy_clean.csv',\n 'male_teams_clean.csv',\n 'female_players_legacy_clean.csv',\n 'male_players_clean.csv']"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}